apiVersion: v1
kind: Pod
metadata:
  name: optimum-tpu-llama3-8b
spec:
  containers:
  - name: optimum-tpu-llama3-8b
    image: gcr.io/tpu-vm-gke-testing/ricliu-optimum-tpu:20240716
    command: ["/bin/sh", "-c", "pip uninstall -y transformers && huggingface-cli login --token $HF_TOKEN && mkdir fork && cd fork && git clone https://hf_token@github.com/wenxindongwork/optimum-tpu.git && cd optimum-tpu && pip install -e . && python kubernetes/finetune_llama3_8b.py"]
    ports:
    - containerPort: 80
    env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: huggingface
          key: token.txt
    resources:
      limits:
        cpu: "8"
        ephemeral-storage: 30Gi
        google.com/tpu: "4"
        memory: 200G
      requests:
        cpu: "8"
        ephemeral-storage: 30Gi
        google.com/tpu: "4"
        memory: 200G
  nodeSelector:
    cloud.google.com/gke-tpu-topology: 2x4
    cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
