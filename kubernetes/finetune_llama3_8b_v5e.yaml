apiVersion: v1
kind: Pod
metadata:
  name: optimum-tpu-llama3-8b
spec:
  containers:
  - name: optimum-tpu-llama3-8b
    image: gcr.io/tpu-vm-gke-testing/ricliu-optimum-tpu:20240716
    command: ["/bin/sh", "-c", "huggingface-cli login --token $HF_TOKEN && mkdir fork && cd fork && git clone 'https://${GIT_TOKEN}@github.com/wenxindongwork/optimum-tpu.git' && cd optimum-tpu && pip install -e . && PT_XLA_DEBUG=1 XLA_IR_DEBUG=1 XLA_HLO_DEBUG=1 XLA_SAVE_TENSORS_FMT='text' XLA_SAVE_TENSORS_FILE='/tmp/save1.ir' python kubernetes/finetune_llama3_8b.py"]
    # command: 
    #   - "sleep"
    #   - "604800"
    imagePullPolicy: Always
    ports:
    - containerPort: 80
    env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: huggingface
          key: token.txt
    - name: GIT_TOKEN
      valueFrom: 
        secretKeyRef:
          name: github
          key: git_token.txt
    resources:
      limits:
        cpu: "8"
        ephemeral-storage: 30Gi
        google.com/tpu: "8"
        memory: 200G
      requests:
        cpu: "8"
        ephemeral-storage: 30Gi
        google.com/tpu: "8"
        memory: 200G
  nodeSelector:
    cloud.google.com/gke-tpu-topology: 2x4
    cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
    #iam.gke.io/gke-metadata-server-enabled: "true"
