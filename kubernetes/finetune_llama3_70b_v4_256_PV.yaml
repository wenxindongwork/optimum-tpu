apiVersion: v1
kind: Service
metadata:
  name: tpu-worker-svc
  labels:
    app: tpu-worker
spec:
  clusterIP: None
  selector:
    app: tpu-worker
---
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-optimum-tpu
spec:
  backoffLimit: 0
  completions: 32
  parallelism: 32
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: tpu-worker
    spec:
      subdomain: tpu-worker-svc
      restartPolicy: Never
      volumes:
      - name: persistent-disk
        persistentVolumeClaim:
          claimName: wenxindong-optimum-tpu-claim
          readOnly: true
      containers:
      - name: optimum-tpu
        image: gcr.io/tpu-vm-gke-testing/ricliu-optimum-tpu:20240716
        # command: ["/bin/sh", "-c", "pip uninstall -y transformers && huggingface-cli login --token $HF_TOKEN && mkdir fork && cd fork && git clone 'https://${GIT_TOKEN}@github.com/wenxindongwork/optimum-tpu.git && cd optimum-tpu' && pip install -e . && python kubernetes/finetune_llama3_70b.py"]
        # command: ["/bin/sh", "-c", "huggingface-cli login --token $HF_TOKEN && mkdir fork && cd fork && git clone 'https://${GIT_TOKEN}@github.com/wenxindongwork/optimum-tpu.git' && python optimum-tpu/kubernetes/finetune_llama3_70b.py"]
        command:
          - "sleep"
          - "604800"
        ports:
        - containerPort: 8471
        volumeMounts:
        - mountPath: "/usr/share/storage"
          name: persistent-disk
          readOnly: true
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface
              key: token.txt
        - name: GIT_TOKEN
          valueFrom: 
            secretKeyRef:
              name: github
              key: git_token.txt
        resources:
          limits:
            cpu: "8"
            ephemeral-storage: 400Gi
            google.com/tpu: "4"
            memory: 400G
          requests:
            cpu: "8"
            ephemeral-storage: 250Gi
            google.com/tpu: "4"
            memory: 200G
      nodeSelector:
        cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice
        cloud.google.com/gke-tpu-topology: 4x4x8
